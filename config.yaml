# General (hyper)parameters
params:
  seed: 777
  do_train: True
  do_eval: True
  do_predict: True
  src: en_XX # See https://huggingface.co/facebook/mbart-large-50
  tgt: fa_IR # See https://huggingface.co/facebook/mbart-large-50
  lower_case: True # Tokenizer
  normalization: True # Tokenizer
  checkpoint: facebook/mbart-large-50
  max_len: 100
  # dataloader (bs = batch size)
  train_bs: 8 
  dev_bs: 8 
  test_bs: 8
  # bs per device
  device_train_bs: 8
  device_eval_bs: 8
  epoch: 2
   # log, save, and evaluate every n steps
  logging_step: 1000
  save_steps: 1000
  eval_steps: 1000
  save: 30
  early_stop: 5
  weight_decay: 0
  warmup_steps: 0
  fp16: False # only can be used on CUDA devices
  lr: 5e-06
  evaluation: steps
  best_model: True
  optim: adafactor  #adafactor, adamw_torch default:adamw_hf
  hidden_dropout_prob: 0.1
  # eval metric
  metric_best_model: eval_loss
  predict_with_generate: True # Otherwise you get: out of range integral type conversion attempted
  use_mps_device: True # Using Apple Silicon chip
# Dataset setup
dataset:
  train_path: datasets/EN-FA/train.csv
  dev_path: datasets/EN-FA/dev.csv
  test_path: datasets/EN-FA/test.csv
# Splitter setup
splitter:
  active: True
  path: datasets/EN-FA/en-fa.txt
  split: 0.003 
# MLFlow config
mlflow:
  exp_name: EN-FA-MT
  params: "_1"
